performance(x.rp.prob.rocr,"auc")@y.values
AUCs[1]<-unlist(performance(x.rp.prob.rocr,"auc")@y.values)
d1 <- data.frame(x1=x.rp.perf@x.values[[1]], y1=x.rp.perf@y.values[[1]], Methods="Recursive Partitioning Trees")
# ctree
x.ct.prob.rocr <- prediction(x.ct.prob[testSet,], Y[testSet])
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
performance(x.ct.prob.rocr,"auc")@y.values
AUCs[2]<-unlist(performance(x.ct.prob.rocr,"auc")@y.values)
d2 <- data.frame(x1=x.ct.perf@x.values[[1]], y1=x.ct.perf@y.values[[1]], Methods="CITs")
# cforest
x.cf.prob.rocr <- prediction(x.cf.prob[testSet,], Y[testSet])
x.cf.perf <- performance(x.cf.prob.rocr, "tpr","fpr")
performance(x.cf.prob.rocr,"auc")@y.values
AUCs[3]<-unlist(performance(x.cf.prob.rocr,"auc")@y.values)
d3 <- data.frame(x1=x.cf.perf@x.values[[1]], y1=x.cf.perf@y.values[[1]], Methods="Random Forest with CITs")
# bagging
x.ip.prob.rocr <- prediction(x.ip.prob[testSet,2], Y[testSet])
x.ip.perf <- performance(x.ip.prob.rocr, "tpr","fpr")
performance(x.ip.prob.rocr,"auc")@y.values
AUCs[4]<-unlist(performance(x.ip.prob.rocr,"auc")@y.values)
d4 <- data.frame(x1=x.ip.perf@x.values[[1]], y1=x.ip.perf@y.values[[1]], Methods="Bagging")
# svm
x.svm.prob.rocr <- prediction(x.svm.prob[testSet,"1"], Y[testSet])
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
performance(x.svm.prob.rocr,"auc")@y.values
AUCs[5]<-unlist(performance(x.svm.prob.rocr,"auc")@y.values)
d5 <- data.frame(x1=x.svm.perf@x.values[[1]], y1=x.svm.perf@y.values[[1]], Methods="SVMs with Gaussian Kernel")
# svm, linear
x.svm.l.prob.rocr <- prediction(x.svm.l.prob[testSet,"1"], Y[testSet])
x.svm.l.perf <- performance(x.svm.l.prob.rocr, "tpr","fpr")
performance(x.svm.l.prob.rocr,"auc")@y.values
AUCs[6]<-unlist(performance(x.svm.l.prob.rocr,"auc")@y.values)
d6 <- data.frame(x1=x.svm.l.perf@x.values[[1]], y1=x.svm.l.perf@y.values[[1]], Methods="SVMs with Linear Kernel")
# svm, polynomial
x.svm.p.prob.rocr <- prediction(x.svm.p.prob[testSet,"1"], Y[testSet])
x.svm.p.perf <- performance(x.svm.p.prob.rocr, "tpr","fpr")
performance(x.svm.p.prob.rocr,"auc")@y.values
AUCs[7]<-unlist(performance(x.svm.p.prob.rocr,"auc")@y.values)
d7 <- data.frame(x1=x.svm.p.perf@x.values[[1]], y1=x.svm.p.perf@y.values[[1]], Methods="SVMs with Polynomial Kernel")
# svm, sigmoid
x.svm.s.prob.rocr <- prediction(x.svm.s.prob[testSet,"1"], Y[testSet])
x.svm.s.perf <- performance(x.svm.s.prob.rocr, "tpr","fpr")
performance(x.svm.s.prob.rocr,"auc")@y.values
AUCs[8]<-unlist(performance(x.svm.s.prob.rocr,"auc")@y.values)
d8 <- data.frame(x1=x.svm.s.perf@x.values[[1]], y1=x.svm.s.perf@y.values[[1]], Methods="SVMs with Sigmoid Kernel")
# decision trees
x.dt.prob.rocr <- prediction(x.dt.prob[testSet,2], Y[testSet])
x.dt.perf <- performance(x.dt.prob.rocr, "tpr","fpr")
performance(x.dt.prob.rocr,"auc")@y.values
AUCs[9]<-unlist(performance(x.dt.prob.rocr,"auc")@y.values)
d9 <- data.frame(x1=x.dt.perf@x.values[[1]], y1=x.dt.perf@y.values[[1]], Methods="Decision Trees")
# random forest
x.rf.prob.rocr <- prediction(x.rf.prob[testSet,2], Y[testSet])
x.rf.perf <- performance(x.rf.prob.rocr, "tpr","fpr")
performance(x.rf.prob.rocr,"auc")@y.values
AUCs[10]<-unlist(performance(x.rf.prob.rocr,"auc")@y.values)
d10 <- data.frame(x1=x.rf.perf@x.values[[1]], y1=x.rf.perf@y.values[[1]], Methods="Random Forest")
# naive Bayes
x.nb.prob.rocr <- prediction(x.nb.prob[testSet,2], Y[testSet])
x.nb.perf <- performance(x.nb.prob.rocr, "tpr","fpr")
performance(x.nb.prob.rocr,"auc")@y.values
AUCs[11]<-unlist(performance(x.nb.prob.rocr,"auc")@y.values)
d11 <- data.frame(x1=x.nb.perf@x.values[[1]], y1=x.nb.perf@y.values[[1]], Methods="NB")
# naive Bayes with Laplace smoothing
x.nb.l.prob.rocr <- prediction(x.nb.l.prob[testSet,2], Y[testSet])
x.nb.l.perf <- performance(x.nb.l.prob.rocr, "tpr","fpr")
performance(x.nb.l.prob.rocr,"auc")@y.values
AUCs[12]<-unlist(performance(x.nb.l.prob.rocr,"auc")@y.values)
d12 <- data.frame(x1=x.nb.l.perf@x.values[[1]], y1=x.nb.l.perf@y.values[[1]], Methods="NB with Laplace Smoothing")
percentageFinish<-percentageFinish+2
print(paste("Finished model evaluation",round(percentageFinish,0),sep=","))
write(paste("Finished model evaluation",round(percentageFinish,0),sep=","),milestonesFileName,append=T)
## output decision tree
figureFileName<-paste("public/sessions/",sessionID,"/decisionTree.png",sep="")
png(filename=figureFileName, width=1000, height=500)
plot(x.rp, uniform=TRUE)
text(x.rp, use.n=TRUE, all=TRUE, cex=.8)
dev.off()
## output ROC curves
figureFileName<-paste("public/sessions/",sessionID,"/roc.png",sep="")
png(filename=figureFileName, width=1000, height=500)
ggplot() +
geom_path(aes(x1, y1, colour=Methods), d1) +
geom_path(aes(x1, y1, colour=Methods), d2) +
geom_line(aes(x1, y1, colour=Methods), d3) +
geom_line(aes(x1, y1, colour=Methods), d4) +
geom_line(aes(x1, y1, colour=Methods), d5) +
geom_line(aes(x1, y1, colour=Methods), d6) +
geom_line(aes(x1, y1, colour=Methods), d7) +
geom_line(aes(x1, y1, colour=Methods), d8) +
geom_line(aes(x1, y1, colour=Methods), d9) +
geom_line(aes(x1, y1, colour=Methods), d10) +
geom_line(aes(x1, y1, colour=Methods), d11) +
geom_line(aes(x1, y1, colour=Methods), d12) +
geom_line(aes(x1, x1), colour="#000000", d12) +
theme(plot.title = element_text(size=24,face="bold"),legend.text=element_text(size=16), legend.title=element_blank(), axis.text=element_text(size=18), axis.title=element_text(size=24,face="bold")) +
xlab("1 - Specificity") +
ylab("Sensitivity") +
ggtitle("ROC Curves for Machine Learning Models")
# Close and save the PNG file.
dev.off()
# output AUC
write.table(AUCs,paste("public/sessions/",sessionID,"/AUCs.txt",sep=""),quote=F, sep="\t",row.names=F, col.names=F)
percentageFinish<-100
print(paste("Completed!",percentageFinish,sep=","))
write(paste("Completed!",percentageFinish,sep=","),milestonesFileName,append=T)
# binaryClassification.R
#
# train and test binary classifiers
# plot ROC for classifiers
#
# param 1: tumor type
# param 2: data type (RNAseq, proteomics, etc)
# param 3: prediction target
# param 4: partition type (random, batch)
# param 5: partition seed (random seed, name of batch file)
# param 6: percentage of data in the training set
# param 7: feature selection method
# param 8: number of features
# param 9: session ID
#
# Kun-Hsing Yu
# April 15, 2016
rm(list=ls())
library(FSelector)
library(cvTools)
library(dplyr)
library(rpart)
library(party)
library(ipred)
library(e1071)
library(randomForest)
library(ROCR)
library(ggplot2)
setwd("~/Movies/OASISPRO/oasispro")
sysargs<-c("acc","proteomics","gender","kfold","10","0.7","infog","10","8763207091208097")
print (sysargs)
tumorType<-sysargs[1]
dataType<-sysargs[2]
predictionTarget<-sysargs[3]
partitionType<-sysargs[4]
if (partitionType == "random") { # random
partitionSeed<-as.numeric(sysargs[5])
trainingPercentage<-as.numeric(sysargs[6])
nRandGroups<-20
} else if (partitionType == "batch") { # batch
partitionSeed<-(-1)
selectedBatchesFile<-read.table(paste("public/sessions/",sysargs[9],"/",sysargs[5],sep=""), sep="")
selectedBatches<-selectedBatchesFile[,1]
trainingPercentage<-(-1)
nRandGroups<-1
} else if (partitionType == "kfold") {
nFolds<-as.numeric(sysargs[5])
}
featureSelectionMethod<-sysargs[7]
numFeatures<-as.numeric(sysargs[8])
sessionID<-sysargs[9]
AUCs<-rep(0,12)
# tumorType<-"chol"
# dataType<-"rnaseq"
# predictionTarget<-"pathologic_stage"
# partitionType<-"random"
# if (partitionType == "random") {
#   partitionSeed<-1
#   trainingPercentage<-0.7
#   nRandGroups<-1
# } else {
#   partitionSeed<-(-1)
#   selectedBatchesFile<-read.table(sysargs[5], sep="")
#   selectedBatches<-selectedBatchesFile[,1]
#   trainingPercentage<-(-1)
#   nRandGroups<-1
# }
# featureSelectionMethod<-"infog"
# numFeatures<-20
print (featureSelectionMethod)
print (numFeatures)
milestonesFileName<-paste("public/sessions/",sessionID,"/milestones.txt",sep="")
percentageFinish<-0
# read files
omicsFile<-read.table(paste("../data/", tumorType, "_", dataType, ".txt", sep=""), stringsAsFactors=F, sep=",")
percentageFinish<-2
print(paste("Finished reading omics file",percentageFinish,sep=","))
write(paste("Finished reading omics file",percentageFinish,sep=","),milestonesFileName)
omicsIDFile<-read.table(paste("../data/", tumorType, "_", dataType, "_ids.txt", sep=""), stringsAsFactors=F, sep="")
omicsFile<-omicsFile[substr(omicsIDFile[,1],14,15)=="01",]
omicsIDFile<-omicsIDFile[substr(omicsIDFile[,1],14,15)=="01",]
omicsID<-substr(omicsIDFile, 1, 12)
omicsNameFile<-t(read.table(paste("../data/", tumorType, "_", dataType, "_elemid.txt", sep=""), stringsAsFactors=F, sep=","))
clinicalFile<-read.table(paste("../data/", "nationwidechildrens.org_clinical_patient_", tumorType, ".txt", sep=""), stringsAsFactors = F, sep="\t", quote="")
clinical<-clinicalFile[4:dim(clinicalFile)[1],]
colnames(clinical)<-clinicalFile[2,]
clinicalID<-clinical[,2]
IDintersection<-intersect(omicsID, clinicalID)
omics<-omicsFile[(omicsID %in% IDintersection),]
clinical<-clinical[(clinicalID %in% IDintersection),]
intersectIDs<-omicsID[(omicsID %in% IDintersection)]
YFile<-clinical[,predictionTarget]
uniqueValues<-unique(as.factor(YFile))
YFile[substr(YFile,1,2)=="[N"]<-NA
YFile[substr(YFile,nchar(YFile),nchar(YFile))=="X"]<-NA
threshold<-median(as.numeric(as.factor(YFile)),na.rm=T)
Y<-YFile[!is.na(YFile)]
Y<-(as.numeric(as.factor(Y))>threshold)
Y<-ifelse(Y=="TRUE", 1, 0)
Y<-as.factor(Y)
Xall<-omics[!is.na(YFile),]
iDs<-intersectIDs[!is.na(YFile)]
percentageFinish<-5
print(paste("Finished building clinical matrix",percentageFinish,sep=","))
write(paste("Finished building clinical matrix",percentageFinish,sep=","),milestonesFileName,append=T)
if (partitionType == "random"){ # random
trainingSet<-sample(length(Y), floor(length(Y)*trainingPercentage), replace=F)
testSet<-which(!(1:length(Y) %in% trainingSet))
nFolds<-1
} else if (partitionType == "kfold"){ # k fold
folds<-cvFolds(length(Y), K=nFolds)
} else if (partitionType == "loocv"){ # LOOCV
nFolds<-length(Y)
folds<-cvFolds(length(Y), K=length(Y))
} else { # batch
trainingSet<-which(substr(iDs, 6, 7) %in% selectedBatches)
testSet<-which(!(1:length(Y) %in% trainingSet))
nFolds<-1
}
percentageStep<-90/nFolds
x.rp.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.ct.prob<-as.data.frame(matrix(ncol=1, nrow=length(Y)))
x.cf.prob<-as.data.frame(matrix(ncol=1, nrow=length(Y)))
x.ip.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.svm.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.svm.l.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.svm.p.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.svm.s.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.dt.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.rf.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.nb.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
x.nb.l.prob<-as.data.frame(matrix(ncol=2, nrow=length(Y)))
for (i in 1:nFolds) {
if (nFolds>1){ # kfold or loocv
trainingSet<-folds$subsets[,1][folds$which != i]
testSet<-which(!(1:length(Y) %in% trainingSet))
print(paste("Fold: ",i,sep=""))
}
## feature selection
YTrain<-Y[trainingSet]
XTune<-cbind(Xall[trainingSet,],YTrain)
if (featureSelectionMethod == "infog"){ # information gain
weights <- information.gain(YTrain~., XTune)
weightsOutput<-cbind(omicsNameFile,weights)
weightsOutput<-weightsOutput[order(weightsOutput[,2],decreasing=T),]
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-Xall[,subsetFeatures]
} else if (featureSelectionMethod == "gainr"){ # gain ratio
weights <- gain.ratio(YTrain~., XTune)
weightsOutput<-cbind(omicsNameFile,weights)
weightsOutput<-weightsOutput[order(weightsOutput[,2],decreasing=T),]
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-Xall[,subsetFeatures]
} else if (featureSelectionMethod == "symu"){ # symmetrical uncertainty
weights <- symmetrical.uncertainty(YTrain~., XTune)
weightsOutput<-cbind(omicsNameFile,weights)
weightsOutput<-weightsOutput[order(weightsOutput[,2],decreasing=T),]
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-Xall[,subsetFeatures]
} else if (FALSE){ # consistency
weights <- consistency(YTrain~., XTune)
X<-Xall[,weights]
# chi-squared
weights <- chi.squared(YTrain~., XTune)
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-X[,subsetFeatures]
} else if (FALSE){ # cfs
weights <- cfs(YTrain~., XTune)
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-Xall[,weights]
} else if (featureSelectionMethod == "randf"){ # random forest importance
weights <- random.forest.importance(YTrain~., XTune, importance.type = 1)
weightsOutput<-cbind(omicsNameFile,weights)
weightsOutput<-weightsOutput[order(weightsOutput[,2],decreasing=T),]
subsetFeatures<-cutoff.k(weights, numFeatures)
X<-Xall[,subsetFeatures]
} else { # custom
customFile<-read.table(featureSelectionMethod, sep="")
subsetFeatures<-customFile[,1]
X<-Xall[,subsetFeatures]
}
write.table(weightsOutput,paste("public/sessions/",sessionID,"/featureWeights.txt",sep=""),quote=F, sep=",",row.names=F, col.names=F)
percentageFinish<-percentageFinish+(percentageStep/2)
if (nFolds>1){
print(paste(paste("Fold ",i,": Finished feature selection",sep=""),round(percentageFinish,0),sep=","))
write(paste(paste("Fold ",i,": Finished feature selection",sep=""),round(percentageFinish,0),sep=","),milestonesFileName,append=T)
} else {
print(paste("Finished feature selection",round(percentageFinish,0),sep=","))
write(paste("Finished feature selection",round(percentageFinish,0),sep=","),milestonesFileName,append=T)
}
# fit recursive partitioning tree
x.rp <- rpart(Y[trainingSet]~., data=X[trainingSet,])
x.rp.pred <- predict(x.rp, X[testSet,], type="class")
x.rp.prob[testSet,] <- predict(x.rp, X[testSet,], type="prob")
# fit conditional inference trees
x.ct <- ctree(Y[trainingSet]~., data=X[trainingSet,])
x.ct.pred <- predict(x.ct, X[testSet,])
x.ct.prob[testSet,] <- 1- unlist(treeresponse(x.ct, X[testSet,]), use.names=F)[seq(1,nrow(X[testSet,])*2,2)]
# fit random forest with conditional inference trees
x.cf <- cforest(Y[trainingSet]~., data=X[trainingSet,], control = cforest_unbiased(mtry = ncol(X)-2))
x.cf.pred <- predict(x.cf, newdata=X[testSet,])
x.cf.prob[testSet,] <- 1- unlist(treeresponse(x.cf, X[testSet,]), use.names=F)[seq(1,nrow(X[testSet,])*2,2)]
# fit bagging (bootstrap aggregating)
x.ip <- bagging(Y[trainingSet]~., data=X[trainingSet,])
x.ip.prob[testSet,] <- predict(x.ip, type="prob", newdata=X[testSet,])
# fit svm (support vector machine), radial
XTune<-cbind(X,Y)
x.svm.tune <- tune(svm, Y~., data = XTune[trainingSet,],
ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
tunecontrol = tune.control(sampling = "fix"))
x.svm.tune
x.svm <- svm(Y[trainingSet]~., data = X[trainingSet,], cost=x.svm.tune$best.parameters[2], gamma=x.svm.tune$best.parameters[1], probability = TRUE)
x.svm.prob.tmp <- attr(predict(x.svm, type="prob", newdata=X[testSet,], probability = TRUE), "probabilities")
x.svm.prob[testSet,] <- x.svm.prob.tmp
colnames(x.svm.prob) <- colnames(x.svm.prob.tmp)
# fit svm (support vector machine), linear
XTune<-cbind(X,Y)
x.svm.tune <- tune(svm, Y~., data = XTune[trainingSet,],
ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
tunecontrol = tune.control(sampling = "fix"), kernel = "linear")
x.svm.tune
x.svm.l <- svm(Y[trainingSet]~., data = X[trainingSet,], kernel = "linear", cost=x.svm.tune$best.parameters[2], gamma=x.svm.tune$best.parameters[1], probability = TRUE)
x.svm.l.prob.tmp <- attr(predict(x.svm.l, type="prob", newdata=X[testSet,], probability = TRUE), "probabilities")
x.svm.l.prob[testSet,] <- x.svm.l.prob.tmp
colnames(x.svm.l.prob) <- colnames(x.svm.l.prob.tmp)
# fit svm (support vector machine), polynomial
XTune<-cbind(X,Y)
x.svm.tune <- tune(svm, Y~., data = XTune[trainingSet,],
ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
tunecontrol = tune.control(sampling = "fix"), kernel = "polynomial")
x.svm.tune
x.svm.p <- svm(Y[trainingSet]~., data = X[trainingSet,], kernel = "polynomial", cost=x.svm.tune$best.parameters[2], gamma=x.svm.tune$best.parameters[1], probability = TRUE)
x.svm.p.prob.tmp <- attr(predict(x.svm.p, type="prob", newdata=X[testSet,], probability = TRUE), "probabilities")
x.svm.p.prob[testSet,] <- x.svm.p.prob.tmp
colnames(x.svm.p.prob) <- colnames(x.svm.p.prob.tmp)
# fit svm (support vector machine), sigmoid
XTune<-cbind(X,Y)
x.svm.tune <- tune(svm, Y~., data = XTune[trainingSet,],
ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
tunecontrol = tune.control(sampling = "fix"), kernel = "linear")
x.svm.tune
x.svm.s <- svm(Y[trainingSet]~., data = X[trainingSet,], kernel = "linear", cost=x.svm.tune$best.parameters[2], gamma=x.svm.tune$best.parameters[1], probability = TRUE)
x.svm.s.prob.tmp <- attr(predict(x.svm.l, type="prob", newdata=X[testSet,], probability = TRUE), "probabilities")
x.svm.s.prob[testSet,] <- x.svm.s.prob.tmp
colnames(x.svm.s.prob) <- colnames(x.svm.s.prob.tmp)
# fit decision tree
dep=10
x.dt<-rpart(Y[trainingSet]~.,X[trainingSet,], control=rpart.control(minsplit=0, minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=dep))
x.dt.prob[testSet,] <- predict(x.dt, type="prob", newdata=X[testSet,], probability = TRUE)
# fit random forest
x.rf<-randomForest(X[trainingSet,],Y[trainingSet])
x.rf.prob[testSet,] <- predict(x.rf, type="prob", newdata=X[testSet,], probability = TRUE)
# fit naive Bayes
x.nb<-naiveBayes(X[trainingSet,],Y[trainingSet])
x.nb.prob[testSet,] <- predict(x.nb, type="raw", newdata=X[testSet,], probability = TRUE)
# fit naive Bayes with laplace smoothing
x.nb.l<-naiveBayes(X[trainingSet,],Y[trainingSet], laplace = 3)
x.nb.l.prob[testSet,] <- predict(x.nb.l, type="raw", newdata=X[testSet,], probability = TRUE)
percentageFinish<-percentageFinish+(percentageStep/2)
if (nFolds>1){
print(paste(paste("Fold ",i,": Finished prediction",sep=""),round(percentageFinish,0),sep=","))
write(paste(paste("Fold ",i,": Finished prediction",sep=""),round(percentageFinish,0),sep=","),milestonesFileName,append=T)
} else {
print(paste("Finished prediction",round(percentageFinish,0),sep=","))
write(paste("Finished prediction",round(percentageFinish,0),sep=","),milestonesFileName,append=T)
}
}
## plot ROC curves
if (nFolds>1){ # kfold, LOOCV
testSet=1:length(Y)
}
# rpart
x.rp.prob.rocr <- prediction(x.rp.prob[testSet,2], Y[testSet])
x.rp.perf <- performance(x.rp.prob.rocr, "tpr","fpr")
performance(x.rp.prob.rocr,"auc")@y.values
AUCs[1]<-unlist(performance(x.rp.prob.rocr,"auc")@y.values)
d1 <- data.frame(x1=x.rp.perf@x.values[[1]], y1=x.rp.perf@y.values[[1]], Methods="Recursive Partitioning Trees")
# ctree
x.ct.prob.rocr <- prediction(x.ct.prob[testSet,], Y[testSet])
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
performance(x.ct.prob.rocr,"auc")@y.values
AUCs[2]<-unlist(performance(x.ct.prob.rocr,"auc")@y.values)
d2 <- data.frame(x1=x.ct.perf@x.values[[1]], y1=x.ct.perf@y.values[[1]], Methods="CITs")
# cforest
x.cf.prob.rocr <- prediction(x.cf.prob[testSet,], Y[testSet])
x.cf.perf <- performance(x.cf.prob.rocr, "tpr","fpr")
performance(x.cf.prob.rocr,"auc")@y.values
AUCs[3]<-unlist(performance(x.cf.prob.rocr,"auc")@y.values)
d3 <- data.frame(x1=x.cf.perf@x.values[[1]], y1=x.cf.perf@y.values[[1]], Methods="Random Forest with CITs")
# bagging
x.ip.prob.rocr <- prediction(x.ip.prob[testSet,2], Y[testSet])
x.ip.perf <- performance(x.ip.prob.rocr, "tpr","fpr")
performance(x.ip.prob.rocr,"auc")@y.values
AUCs[4]<-unlist(performance(x.ip.prob.rocr,"auc")@y.values)
d4 <- data.frame(x1=x.ip.perf@x.values[[1]], y1=x.ip.perf@y.values[[1]], Methods="Bagging")
# svm
x.svm.prob.rocr <- prediction(x.svm.prob[testSet,"1"], Y[testSet])
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
performance(x.svm.prob.rocr,"auc")@y.values
AUCs[5]<-unlist(performance(x.svm.prob.rocr,"auc")@y.values)
d5 <- data.frame(x1=x.svm.perf@x.values[[1]], y1=x.svm.perf@y.values[[1]], Methods="SVMs with Gaussian Kernel")
# svm, linear
x.svm.l.prob.rocr <- prediction(x.svm.l.prob[testSet,"1"], Y[testSet])
x.svm.l.perf <- performance(x.svm.l.prob.rocr, "tpr","fpr")
performance(x.svm.l.prob.rocr,"auc")@y.values
AUCs[6]<-unlist(performance(x.svm.l.prob.rocr,"auc")@y.values)
d6 <- data.frame(x1=x.svm.l.perf@x.values[[1]], y1=x.svm.l.perf@y.values[[1]], Methods="SVMs with Linear Kernel")
# svm, polynomial
x.svm.p.prob.rocr <- prediction(x.svm.p.prob[testSet,"1"], Y[testSet])
x.svm.p.perf <- performance(x.svm.p.prob.rocr, "tpr","fpr")
performance(x.svm.p.prob.rocr,"auc")@y.values
AUCs[7]<-unlist(performance(x.svm.p.prob.rocr,"auc")@y.values)
d7 <- data.frame(x1=x.svm.p.perf@x.values[[1]], y1=x.svm.p.perf@y.values[[1]], Methods="SVMs with Polynomial Kernel")
# svm, sigmoid
x.svm.s.prob.rocr <- prediction(x.svm.s.prob[testSet,"1"], Y[testSet])
x.svm.s.perf <- performance(x.svm.s.prob.rocr, "tpr","fpr")
performance(x.svm.s.prob.rocr,"auc")@y.values
AUCs[8]<-unlist(performance(x.svm.s.prob.rocr,"auc")@y.values)
d8 <- data.frame(x1=x.svm.s.perf@x.values[[1]], y1=x.svm.s.perf@y.values[[1]], Methods="SVMs with Sigmoid Kernel")
# decision trees
x.dt.prob.rocr <- prediction(x.dt.prob[testSet,2], Y[testSet])
x.dt.perf <- performance(x.dt.prob.rocr, "tpr","fpr")
performance(x.dt.prob.rocr,"auc")@y.values
AUCs[9]<-unlist(performance(x.dt.prob.rocr,"auc")@y.values)
d9 <- data.frame(x1=x.dt.perf@x.values[[1]], y1=x.dt.perf@y.values[[1]], Methods="Decision Trees")
# random forest
x.rf.prob.rocr <- prediction(x.rf.prob[testSet,2], Y[testSet])
x.rf.perf <- performance(x.rf.prob.rocr, "tpr","fpr")
performance(x.rf.prob.rocr,"auc")@y.values
AUCs[10]<-unlist(performance(x.rf.prob.rocr,"auc")@y.values)
d10 <- data.frame(x1=x.rf.perf@x.values[[1]], y1=x.rf.perf@y.values[[1]], Methods="Random Forest")
# naive Bayes
x.nb.prob.rocr <- prediction(x.nb.prob[testSet,2], Y[testSet])
x.nb.perf <- performance(x.nb.prob.rocr, "tpr","fpr")
performance(x.nb.prob.rocr,"auc")@y.values
AUCs[11]<-unlist(performance(x.nb.prob.rocr,"auc")@y.values)
d11 <- data.frame(x1=x.nb.perf@x.values[[1]], y1=x.nb.perf@y.values[[1]], Methods="NB")
# naive Bayes with Laplace smoothing
x.nb.l.prob.rocr <- prediction(x.nb.l.prob[testSet,2], Y[testSet])
x.nb.l.perf <- performance(x.nb.l.prob.rocr, "tpr","fpr")
performance(x.nb.l.prob.rocr,"auc")@y.values
AUCs[12]<-unlist(performance(x.nb.l.prob.rocr,"auc")@y.values)
d12 <- data.frame(x1=x.nb.l.perf@x.values[[1]], y1=x.nb.l.perf@y.values[[1]], Methods="NB with Laplace Smoothing")
percentageFinish<-percentageFinish+2
print(paste("Finished model evaluation",round(percentageFinish,0),sep=","))
write(paste("Finished model evaluation",round(percentageFinish,0),sep=","),milestonesFileName,append=T)
## output decision tree
figureFileName<-paste("public/sessions/",sessionID,"/decisionTree.png",sep="")
png(filename=figureFileName, width=1000, height=500)
plot(x.rp, uniform=TRUE)
text(x.rp, use.n=TRUE, all=TRUE, cex=.8)
dev.off()
## output ROC curves
figureFileName<-paste("public/sessions/",sessionID,"/roc.png",sep="")
png(filename=figureFileName, width=1000, height=500)
ggplot() +
geom_path(aes(x1, y1, colour=Methods), d1) +
geom_path(aes(x1, y1, colour=Methods), d2) +
geom_line(aes(x1, y1, colour=Methods), d3) +
geom_line(aes(x1, y1, colour=Methods), d4) +
geom_line(aes(x1, y1, colour=Methods), d5) +
geom_line(aes(x1, y1, colour=Methods), d6) +
geom_line(aes(x1, y1, colour=Methods), d7) +
geom_line(aes(x1, y1, colour=Methods), d8) +
geom_line(aes(x1, y1, colour=Methods), d9) +
geom_line(aes(x1, y1, colour=Methods), d10) +
geom_line(aes(x1, y1, colour=Methods), d11) +
geom_line(aes(x1, y1, colour=Methods), d12) +
geom_line(aes(x1, x1), colour="#000000", d12) +
theme(plot.title = element_text(size=24,face="bold"),legend.text=element_text(size=16), legend.title=element_blank(), axis.text=element_text(size=18), axis.title=element_text(size=24,face="bold")) +
xlab("1 - Specificity") +
ylab("Sensitivity") +
ggtitle("ROC Curves for Machine Learning Models")
# Close and save the PNG file.
dev.off()
# output AUC
write.table(AUCs,paste("public/sessions/",sessionID,"/AUCs.txt",sep=""),quote=F, sep="\t",row.names=F, col.names=F)
percentageFinish<-100
print(paste("Completed!",percentageFinish,sep=","))
write(paste("Completed!",percentageFinish,sep=","),milestonesFileName,append=T)
library(superpc)
set.seed(464)
x<-matrix(rnorm(1000*100),ncol=100)
v1<- svd(x[1:80,])$v[,1]
y<-2+5*v1+ .05*rnorm(100)
xtest<-x
ytest<-2+5*v1+ .05*rnorm(100)
censoring.status<- sample(c(rep(1,80),rep(0,20)))
censoring.status.test<- sample(c(rep(1,80),rep(0,20)))
featurenames <- paste("feature",as.character(1:1000),sep="")
featurenames
xtest
data<-list(x=x,y=y, censoring.status= censoring.status, featurenames=featurenames)
data.test<-list(x=xtest,y=ytest, censoring.status=censoring.status.test, featurenames= featurenames)
train.obj<- superpc.train(data, type="survival")
cv.obj<-superpc.cv(train.obj, data)
#plot the cross-validation curves. From this plot we see that the 1st
# principal component is significant and the best threshold  is around 0.7
superpc.plotcv(cv.obj)
lrtest.obj<-superpc.lrtest.curv(train.obj, data,data.test)
superpc.plot.lrtest(lrtest.obj)
fit.cts<- superpc.predict(train.obj, data, data.test, threshold=0.7, n.components=3, prediction.type="continuous")
superpc.fit.to.outcome(train.obj, data.test, fit.cts$v.pred)
fit.groups<- superpc.predict(train.obj, data, data.test, threshold=0.7, n.components=1, prediction.type="discrete")
superpc.fit.to.outcome(train.obj, data.test, fit.groups$v.pred)
plot(survfit(Surv(data.test$y,data.test$censoring.status)~fit.groups$v.pred), col=2:3, xlab="time", ylab="Prob survival")
